{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN for image recognition using CIFAR-10 Dataset**\n",
    " * **Optimizer: Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspiration from the official tutorials for PyTorch from YuliyaPylypiv on GitHub\n",
    "Source: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Number of the GPU in use: \", torch.cuda.device_count())\n",
    "print(\"GPU Model: \", torch.cuda.get_device_name(0))\n",
    "\n",
    "#DEFINE THE NETWORK\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32) \n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.dropout = nn.Dropout(p=0.2)  #Dropout with p=0.2\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(torch.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def print_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            print(f\"Layer: {name}, Number of Weights: {param.numel()}\")\n",
    "\n",
    "\n",
    "#Load CIFAR-10 and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "#This is the creation of the instance of our CNN\n",
    "model = DeepCNN().to(device)\n",
    "model.print_weights()\n",
    "#model.extract_weights_and_biases()\n",
    "\n",
    "#Initialize the model, loss function, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 50\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "losses = []\n",
    "lr_SGD=0.005\n",
    "ADAM_lr = 0.001\n",
    "\n",
    "#Training model\n",
    "def train_model(model, trainloader, criterion, optimizer_type, learning_rate, device, disable_param_fc, disable_param_fc3):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    model.train()\n",
    "\n",
    "    #Customizable optimizer\n",
    "    if optimizer_type == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr_SGD)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=ADAM_lr)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        if disable_param_fc:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'fc3' not in name:  #Checking if the layer is not fc3\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'fc3' not in name:  #Checking if the layer is not fc3\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        if disable_param_fc3:\n",
    "            for param in model.fc3.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in model.fc3.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if not (disable_param_fc or disable_param_fc3):\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    return epoch_loss, train_accuracy\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, train_acc = train_model(model, trainloader, criterion, optimizer_type= 'SGD', learning_rate= lr_SGD, device=device, disable_param_fc=False, disable_param_fc3=False)\n",
    "    losses.append(epoch_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    #Testing and printing the accuracies\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy.append(100 * correct_test / total_test)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy[-1]:.2f}%, Test Accuracy: {test_accuracy[-1]:.2f}%\")\n",
    "\n",
    "#Plotting the accuracies of Training and Test\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs + 1), test_accuracy, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer: PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Randomize weights and biases\n",
    "import torch.nn.init as init\n",
    "\n",
    "def randomize_weights(model):\n",
    "    with torch.no_grad():\n",
    "        init.xavier_uniform_(model.fc3.weight)\n",
    "        init.constant_(model.fc3.bias, 0.0)\n",
    "\n",
    "randomize_weights(model)\n",
    "\n",
    "#Print to see the changed values\n",
    "print(\"Randomized Last Layer Weights:\")\n",
    "print(model.fc3.weight.data)\n",
    "print(\"Randomized Last Layer Biases:\")\n",
    "print(model.fc3.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by the code from lab 4 of COMM3013, University of Surrey\n",
    "Source: https://surreylearn.surrey.ac.uk/d2l/le/lessons/252875/topics/2851191\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "posMinInit      = -1\n",
    "posMaxInit      = +1\n",
    "VMaxInit        = 2\n",
    "VMinInit        = 0.45\n",
    "populationSize  = 60\n",
    "dimension       = 650\n",
    "interval        = 1\n",
    "iterations      = 50 \n",
    "\n",
    "#Parameter setup\n",
    "\n",
    "wmax = 0.87 #weighting\n",
    "wmin = 0.4 \n",
    "c1   = 2.0\n",
    "c2   = 2.0\n",
    "\n",
    "#Other variables or lists\n",
    "best_particle = None\n",
    "pso_list_best =[]\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(+1.0,)) # -1 is for minimise\n",
    "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
    "# particle rerpresented by list of 5 things\n",
    "# 1. fitness of the particle, \n",
    "# 2. speed of the particle which is also going to be a list, \n",
    "# 3.4. limit of the speed value, \n",
    "# 5. best state the particle has been in so far.\n",
    "\n",
    "\n",
    "def generate(size, smin, smax):\n",
    "    part = creator.Particle(random.uniform(posMinInit, posMaxInit) for _ in range(size)) \n",
    "    part.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(size)]\n",
    "    part.smin = smin #speed clamping values\n",
    "    part.smax = smax\n",
    "    return part\n",
    "\n",
    "\n",
    "#Our function to measure the fitness\n",
    "def part_fit(particle):\n",
    "\n",
    "    #Replace the final layer\n",
    "    #Take the first 640 elements of the list \"particle\" -> to tensor -> to fc weights\n",
    "    individual_tensor = torch.tensor(particle[:640]).to(device)\n",
    "    model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "    #Biases are te last 10 values\n",
    "    individual_tensor_b = torch.tensor(particle[-10:]).to(device)\n",
    "    model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "    model.to(device)\n",
    "\n",
    "    #call the training function\n",
    "    epoch_loss, train_accuracy = train_model(model, trainloader, criterion, optimizer_type= 'SGD', learning_rate= lr_SGD, device=device, disable_param_fc=True, disable_param_fc3=True)   \n",
    "    par_val = train_accuracy\n",
    "    return par_val\n",
    "\n",
    "def updateParticle(part, best, weight):\n",
    "    #implementing speed = 0.7*(weight*speed + c1*r1*(localBestPos-currentPos) + c2*r2*(globalBestPos-currentPos))\n",
    "    #Note that part and part.speed are both lists of size dimension\n",
    "    #hence all multiplies need to apply across lists, so using e.g. map(operator.mul, ...\n",
    "\n",
    "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "\n",
    "    v_r0 = [weight*x for x in part.speed]\n",
    "    v_r1 = [c1*x for x in map(operator.mul, r1, map(operator.sub, part.best, part))] # local best\n",
    "    v_r2 = [c2*x for x in map(operator.mul, r2, map(operator.sub, best, part))] # global best\n",
    "    \n",
    "    part.speed = [0.7*x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
    "\n",
    "#not using clamping\n",
    "#     #clamp limits\n",
    "#     for i, speed in enumerate(part.speed):\n",
    "#         if abs(speed) < part.smin:\n",
    "#             part.speed[i] = math.copysign(part.smin, speed)\n",
    "#         elif abs(speed) > part.smax:\n",
    "#             part.speed[i] = math.copysign(part.smax, speed)\n",
    "            \n",
    "    # update position with speed\n",
    "    part[:] = list(map(operator.add, part, part.speed))\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"particle\", generate, size=dimension, smin=-3, smax=3)\n",
    "#toolbox.register(\"particle\", generate, weights_and_biases=weights_and_biases, smin=-3, smax=3)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
    "toolbox.register(\"update\", updateParticle)\n",
    "toolbox.register(\"evaluate\", benchmarks.sphere) #sphere function is built-in in DEAP\n",
    "toolbox.register(\"part_fit\", part_fit)\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=populationSize) # Population Size\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "\n",
    "    best = None\n",
    "\n",
    "    #begin main loop\n",
    "    for g in range(iterations):\n",
    "        print(\"Iteration number \", g+1)\n",
    "        print(\"###########################################################\")\n",
    "        w = wmax - (wmax-wmin)*g/iterations #decaying inertia weight\n",
    "        \n",
    "        for x_1, part in enumerate(pop):\n",
    "            part.fitness.values = (toolbox.part_fit(part),) #changed to use our own function\n",
    "            print(\"Particle at index\", x_1, \"has a fitness of\", part.fitness.values[0])\n",
    "\n",
    "            \n",
    "            #update local best\n",
    "            if (not part.best) or (part.best.fitness < part.fitness):   #lower fitness is better (minimising)\n",
    "            #   best is None   or  current value is better              #< is overloaded        \n",
    "                part.best = creator.Particle(part)\n",
    "                part.best.fitness.values = part.fitness.values\n",
    "            \n",
    "            #update global best\n",
    "            if (not best) or best.fitness < part.fitness:\n",
    "                best = creator.Particle(part)\n",
    "                best.fitness.values = part.fitness.values\n",
    "                best_particle = best.fitness.values\n",
    "                #Printing the best particle when it gets updated\n",
    "                print(\"New best particle value: \", best.fitness.values[0] )\n",
    "                \n",
    "        for part in pop:\n",
    "            toolbox.update(part, best,w)\n",
    "\n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        # print every interval\n",
    "        if g%interval==0: # interval\n",
    "            logbook.record(gen=g, evals=len(pop), **stats.compile(pop))\n",
    "            print(logbook.stream)\n",
    "            pso_list_best.append(logbook.select(\"max\"))\n",
    "    \n",
    "    print('best particle position is ',best)\n",
    "    return pop, logbook, best\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   bestParticle = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUT best particle into the model and test\n",
    "bestParticle2 = bestParticle[2]\n",
    "individual_tensor = torch.tensor(bestParticle2[:640]).to(device)\n",
    "model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "individual_tensor_b = torch.tensor(bestParticle2[-10:]).to(device)\n",
    "model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "\n",
    "#TEST AGAIN\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy_updated2 = 100 * correct_test / total_test\n",
    "print(f\"PSO Test Accuracy with new weights and biases: {test_accuracy_updated2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our Algorithm Proposed: Hybrid ADAM Particle Swarm Optimization (HAPSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by the code from lab 4 of COMM3013, University of Surrey\n",
    "Source: https://surreylearn.surrey.ac.uk/d2l/le/lessons/252875/topics/2851191\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "posMinInit      = -1\n",
    "posMaxInit      = 1\n",
    "VMaxInit        = 2\n",
    "VMinInit        = 0.45\n",
    "populationSize  = 60\n",
    "dimension       = 650\n",
    "interval        = 1\n",
    "iterations      = 50\n",
    "\n",
    "#Parameter setup\n",
    "\n",
    "wmax = 0.9 #weighting\n",
    "wmin = 0.25\n",
    "c1   = 2.3\n",
    "c2   = 1.7\n",
    "\n",
    "###########################################################\n",
    "#New approach variables needed\n",
    "perc_iter = 0.8\n",
    "#Creation of a new variable that sets 20% of the iterations \n",
    "#these iterations will be used for the second approach to apply ADAM\n",
    "iter_counter = int(iterations*perc_iter)\n",
    "print(\"ITER COUNTER: \" ,iter_counter)\n",
    "\n",
    "ADAM_lr = 0.005\n",
    "\n",
    "#Values needed for the resampling based on last generation improvements\n",
    "best_fit_history = []\n",
    "improvement_threshold = 0.02 #The improvement must be at least 2%\n",
    "generations_to_check = 5 #Number of generations to check for improvement  \n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "#Other variables or lists\n",
    "best_particle = None\n",
    "best = None\n",
    "best_position = None\n",
    "our_algorithm_list_best =[]\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(+1.0,)) # -1 is for minimise\n",
    "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
    "# particle rerpresented by list of 5 things\n",
    "# 1. fitness of the particle,\n",
    "# 2. speed of the particle which is also going to be a list,\n",
    "# 3.4. limit of the speed value,\n",
    "# 5. best state the particle has been in so far.\n",
    "\n",
    "\n",
    "def generate(size, smin, smax):\n",
    "    part = creator.Particle(random.uniform(posMinInit, posMaxInit) for _ in range(size))\n",
    "    part.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(size)]\n",
    "    part.smin = smin #speed clamping values\n",
    "    part.smax = smax\n",
    "    return part\n",
    "\n",
    "\n",
    "#Our function to measure the fitness\n",
    "def part_fit(particle):\n",
    "\n",
    "    #Replace the final layer\n",
    "    #Take the first 640 elements of the list \"particle\" -> to tensor -> to fc3 weights\n",
    "    individual_tensor = torch.tensor(particle[:640]).to(device)\n",
    "    model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "    #Biases are te last 10 values\n",
    "    individual_tensor_b = torch.tensor(particle[-10:]).to(device)\n",
    "    model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "    model.to(device)\n",
    "\n",
    "    #call the training function\n",
    "    epoch_loss, train_accuracy = train_model(model, trainloader, criterion, optimizer_type= 'SGD', learning_rate= lr_SGD, device=device, disable_param_fc=True, disable_param_fc3=True)\n",
    "    par_val = train_accuracy\n",
    "    return par_val\n",
    "\n",
    "\n",
    "#USED FOR ADAM: Our function to measure the fitness\n",
    "def part_fit2(particle):\n",
    "\n",
    "    #Replace the final layer\n",
    "    #Take the first 640 elements of the list \"particle\" -> to tensor -> to fc3 weights\n",
    "    individual_tensor = torch.tensor(particle[:640]).to(device)\n",
    "    model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "    #Biases are te last 10 values\n",
    "    individual_tensor_b = torch.tensor(particle[-10:]).to(device)\n",
    "    model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "    model.to(device)\n",
    "\n",
    "    #call the training function\n",
    "    epoch_loss, train_accuracy = train_model(model, trainloader, criterion, optimizer_type= 'Adam', learning_rate= ADAM_lr, device=device, disable_param_fc=True, disable_param_fc3=False)\n",
    "    par_val = train_accuracy\n",
    "    return par_val\n",
    "\n",
    "def updateParticle(part, best, weight):\n",
    "    #implementing speed = 0.7*(weight*speed + c1*r1*(localBestPos-currentPos) + c2*r2*(globalBestPos-currentPos))\n",
    "    #Note that part and part.speed are both lists of size dimension\n",
    "    #hence all multiplies need to apply across lists, so using e.g. map(operator.mul, ...\n",
    "\n",
    "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "\n",
    "    v_r0 = [weight*x for x in part.speed]\n",
    "    v_r1 = [c1*x for x in map(operator.mul, r1, map(operator.sub, part.best, part))] # local best\n",
    "    v_r2 = [c2*x for x in map(operator.mul, r2, map(operator.sub, best, part))] # global best\n",
    "    \n",
    "    part.speed = [0.7*x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
    "\n",
    "    part[:] = list(map(operator.add, part, part.speed))\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"particle\", generate, size=dimension, smin=-3, smax=3)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
    "toolbox.register(\"update\", updateParticle)\n",
    "toolbox.register(\"part_fit\", part_fit)\n",
    "toolbox.register(\"part_fit2\", part_fit2)\n",
    "\n",
    "#DISCLAIMER: during one of our runs we encountered a problem regarding an erroneous conversion\n",
    "#in the datatype Long, which we did not address due to lack of time\n",
    "#Due to the stochasticity of this algorithm, the problem may or may not manifest\n",
    "\n",
    "def main():\n",
    "    best_position=None\n",
    "    best=None\n",
    "    reset = 0\n",
    "\n",
    "    pop = toolbox.population(n=populationSize) # Population Size\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "\n",
    "    best = None\n",
    "\n",
    "    #begin main loop\n",
    "    for g in range(iterations):\n",
    "        reset += 1\n",
    "        print(\"Iteration number \", g+1)\n",
    "        print(\"###########################################################\")\n",
    "        w = wmax - (wmax-wmin)*g/iterations #decaying inertia weight\n",
    "\n",
    "        for x_1, part in enumerate(pop):\n",
    "         \n",
    "\n",
    "            part.fitness.values = (toolbox.part_fit(part),) #changed to use our own function\n",
    "            print(\"Particle at index\", x_1, \"has a fitness of\", part.fitness.values[0])\n",
    "\n",
    "\n",
    "            #update local best\n",
    "            if (not part.best) or (part.best.fitness < part.fitness):   #lower fitness is better (minimising)\n",
    "            #   best is None   or  current value is better              #< is overloaded\n",
    "                part.best = creator.Particle(part)\n",
    "                part.best.fitness.values = part.fitness.values\n",
    "\n",
    "            #update global best\n",
    "            if (not best) or best.fitness < part.fitness:\n",
    "                best = creator.Particle(part)\n",
    "                best.fitness.values = part.fitness.values\n",
    "                best_particle = best.fitness.values\n",
    "                #Printing the best particle when it gets updated\n",
    "                print(\"New best particle value: \", best.fitness.values[0] )\n",
    "\n",
    "                best_fit_history.append(best.fitness.values[0])\n",
    "            \n",
    "            if len(best_fit_history) > generations_to_check:\n",
    "                last_generations_fitness = best_fit_history[-generations_to_check:]\n",
    "                improvement = max(last_generations_fitness) - min(last_generations_fitness)\n",
    "                \n",
    "                if improvement <= improvement_threshold * max(last_generations_fitness) and reset >= 5:\n",
    "                    reset = 0\n",
    "                    print(\"There has been no significant improvement for the last 5 iterations, Resampling particles\")\n",
    "                    worst_particles = sorted(pop, key=lambda x: x.best.fitness.values[0])[:5]\n",
    "                    for particle in worst_particles:\n",
    "                        #Resetting particles' velocity\n",
    "                        particle.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(dimension)]\n",
    "                        #Using a radom distribution to perturb the position the low fitness particles, instead of random resampling\n",
    "                        dist_perturbation = [random.uniform(-0.15, 0.15) for _ in range(dimension)]\n",
    "                        #Updating particle's position as well\n",
    "                        particle[:] = [max(posMinInit, min(posMaxInit, pos + perturb)) for pos, perturb in zip(particle, dist_perturbation)]\n",
    "\n",
    "                    \n",
    "\n",
    "        if (g == iter_counter):\n",
    "         print(\"ADAM on all Particles of generation \", g+1)\n",
    "\n",
    "        x_2=1\n",
    "        for part in pop:\n",
    "            toolbox.update(part, best,w)\n",
    "\n",
    "\n",
    "            #Our approach is applied in the last 20% of iterations\n",
    "            if (g >= iter_counter):\n",
    "                print(\"ADAM section: \")\n",
    "\n",
    "                #Applying ADAM only on the fc3 layer\n",
    "                adam_fc3 = optim.Adam([\n",
    "                {'params': model.fc3.weight},\n",
    "                {'params': model.fc3.bias} \n",
    "                ], lr=ADAM_lr)\n",
    "                adam_fc3.zero_grad() #reset the gradient\n",
    "                \n",
    "                part.fitness.values = (toolbox.part_fit2(part),)\n",
    "                print(\"Particle at index\", x_2, \"has a fitness of\", part.fitness.values[0])\n",
    "                x_2 += 1\n",
    "\n",
    "                #update local best\n",
    "                if (not part.best) or (part.best.fitness < part.fitness):\n",
    "                #   best is None   or  current value is better             \n",
    "                   part.best = creator.Particle(part)\n",
    "                   part.best.fitness.values = part.fitness.values\n",
    "\n",
    "                #update global best\n",
    "                if (not best) or best.fitness < part.fitness:\n",
    "                    best = creator.Particle(part)\n",
    "                    best.fitness.values = part.fitness.values\n",
    "                    best_particle = best.fitness.values\n",
    "                    #Printing the best particle when it gets updated\n",
    "                    print(\"\\033[92m New best particle with ADAM: \\033[0m\", best.fitness.values[0] )\n",
    "\n",
    "                toolbox.update(part, best,w) #update again\n",
    "                #Then we decrease the temperature only in the last 20% of the generations\n",
    "\n",
    "        if g%interval==0: #Intervallo\n",
    "            logbook.record(gen=g, evals=len(pop), **stats.compile(pop))\n",
    "            print(logbook.stream)\n",
    "            our_algorithm_list_best.append(logbook.select(\"max\"))\n",
    "        \n",
    "        #Best particle position\n",
    "        if (not best) or best.fitness < best.fitness:\n",
    "            best = creator.Particle(part)\n",
    "            best.fitness.values = part.fitness.values\n",
    "            best_position = part[:] \n",
    "\n",
    "    return pop, logbook, best\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   best_particle = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ACCURACY OUR ALGORITHM ON TESTSET\n",
    "final_list= best_particle[2]\n",
    "\n",
    "individual_tensor = torch.tensor(final_list[:640]).to(device)\n",
    "model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "individual_tensor_b = torch.tensor(final_list[-10:]).to(device)\n",
    "model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "\n",
    "#TEST AGAIN\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy_updated2 = 100 * correct_test / total_test\n",
    "print(f\"HAPSO: Test Accuracy with new weights and biases: {test_accuracy_updated2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the final chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_accuracy, label='SGD')\n",
    "plt.plot(pso_list_best[-1], label='PSO')\n",
    "plt.plot(our_algorithm_list_best[-1], label='Our Algorithm')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Accuracy on Training set (%)')\n",
    "plt.title('Comparison between algorithms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSGA-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by the code from lab 6 of COMM3013, University of Surrey\n",
    "Source: https://surreylearn.surrey.ac.uk/d2l/le/lessons/252875/lessons/2851159\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#    This file is part of DEAP.\n",
    "#    This implements the NSGA-II in an easy way because it makes us of DEAP subroutines\n",
    "#    The non dominated sort and crowding distance are solved by a simiple call to DEAP subroutines\n",
    "#    and their implementation is hidden.\n",
    "#\n",
    "#    DEAP is free software: you can redistribute it and/or modify\n",
    "#    it under the terms of the GNU Lesser General Public License as\n",
    "#    published by the Free Software Foundation, either version 3 of\n",
    "#    the License, or (at your option) any later version.\n",
    "#\n",
    "#    DEAP is distributed in the hope that it will be useful,\n",
    "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "#    GNU Lesser General Public License for more details.\n",
    "#\n",
    "#    You should have received a copy of the GNU Lesser General Public\n",
    "#    License along with DEAP. If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "BOUND_LOW, BOUND_UP = -1.0, 1.0\n",
    "NDIM = 650\n",
    "\n",
    "def accuracy_value (individual):\n",
    "    #Take the first 640 elements of the list \"particle\" -> to tensor -> to fc3 weights\n",
    "    individual_tensor = torch.tensor(individual[:640]).to(device)\n",
    "    model.fc3.weight.data = individual_tensor.view(model.fc3.weight.size())\n",
    "    #Biases are te last 10 values\n",
    "    individual_tensor_b = torch.tensor(individual[-10:]).to(device)\n",
    "    model.fc3.bias.data = individual_tensor_b.view(model.fc3.bias.size())\n",
    "    model.to(device)\n",
    "    epoch_loss, train_accuracy = train_model(model, trainloader, criterion, optimizer_type= 'SGD', learning_rate= lr_SGD, device=device, disable_param_fc=True, disable_param_fc3=True)\n",
    "    par_val = train_accuracy\n",
    "\n",
    "    return train_accuracy\n",
    "\n",
    "def gaussian_regulariser (individual):\n",
    "    gaussian_regulariser = 0.0\n",
    "    for parameter in individual:\n",
    "        gaussian_regulariser += parameter**2\n",
    "\n",
    "    return gaussian_regulariser\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "def calcFitness (individual):\n",
    "    f1= accuracy_value(individual) #  accuracy\n",
    "    f2= gaussian_regulariser(individual) #gaussian regulariser\n",
    "    print(\"Accuracy \" + str(f1))\n",
    "    print(\"Gaussian \" + str(f2))\n",
    "    return f1,f2\n",
    "\n",
    "def uniform(low, up, size=None):\n",
    "    try:\n",
    "        return [random.uniform(a, b) for a, b in zip(low, up)]\n",
    "    except TypeError:\n",
    "        return [random.uniform(a, b) for a, b in zip([low] * size, [up] * size)]\n",
    "\n",
    "toolbox.register(\"attr_float\", uniform, BOUND_LOW, BOUND_UP, NDIM)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.attr_float)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", calcFitness)\n",
    "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0)\n",
    "flipProb=1.0/9\n",
    "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=20.0, indpb=1.0/NDIM)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "def main(seed=None):\n",
    "    random.seed(seed)\n",
    "\n",
    "    NGEN = 40\n",
    "    MU = 12\n",
    "    CXPB = 0.9\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", numpy.mean, axis=0)\n",
    "    stats.register(\"std\", numpy.std, axis=0)\n",
    "    stats.register(\"min\", numpy.min, axis=0)\n",
    "    stats.register(\"max\", numpy.max, axis=0)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
    "\n",
    "    pop = toolbox.population(n=MU)\n",
    "\n",
    "\n",
    "    invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "\n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, evals=len(invalid_ind), **record)\n",
    "    print(logbook.stream)\n",
    "\n",
    "\n",
    "    for gen in range(1, NGEN):\n",
    "\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
    "\n",
    "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() <= CXPB:\n",
    "                toolbox.mate(ind1, ind2)\n",
    "\n",
    "            toolbox.mutate(ind1)\n",
    "            toolbox.mutate(ind2)\n",
    "            del ind1.fitness.values, ind2.fitness.values\n",
    "\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "\n",
    "        pop = toolbox.select(pop + offspring, MU)\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "\n",
    "    print(\"Final population hypervolume is %f\" % hypervolume(pop, [11.0, 11.0]))\n",
    "\n",
    "    return pop, logbook\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pop, stats = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.sort(key=lambda x: x.fitness.values)\n",
    "front = numpy.array([ind.fitness.values for ind in pop])\n",
    "plt.scatter(front[:,0], front[:,1], c=\"b\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Gaussian Regulariser')\n",
    "plt.title('NSGA-II plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
